Running combined model...
2025-05-26 12:24:55,844 - INFO - Parsed arguments:
{
    "data_path": "data/sample_100K.csv",
    "output_dir": "logs/training/all/Experiment4/combined",
    "target_column": "tag",
    "important_features": "POSIX_SEQ_WRITES POSIX_SIZE_READ_1K_10K POSIX_SIZE_READ_0_100 POSIX_SIZE_WRITE_100K_1M POSIX_MEM_NOT_ALIGNED POSIX_FILE_NOT_ALIGNED POSIX_SEEKS POSIX_OPENS",
    "similarity_threshold": 0.05,
    "model_type": "combined",
    "gnn_type": "gcn",
    "hidden_dim": 64,
    "num_layers": 2,
    "dropout": 0.1,
    "batch_size": 32,
    "epochs": 100,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "patience": 10,
    "seed": 42,
    "device": "cuda",
    "precomputed_similarity_path": "data/similarity_output_merged_100K.pt"
}
2025-05-26 12:24:55,851 - INFO - Using device: cuda
2025-05-26 12:24:55,851 - INFO - Initialized I/O data processor for data/sample_100K.csv
2025-05-26 12:24:55,851 - INFO - Loading data from data/sample_100K.csv
2025-05-26 12:24:56,312 - INFO - Loaded data with shape (100000, 46)
2025-05-26 12:24:56,312 - INFO - Preprocessing data
2025-05-26 12:24:56,312 - INFO - Initialized feature similarity graph constructor for POSIX_SEQ_WRITES
2025-05-26 12:24:56,312 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_1K_10K
2025-05-26 12:24:56,312 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_0_100
2025-05-26 12:24:56,312 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_WRITE_100K_1M
2025-05-26 12:24:56,312 - INFO - Initialized feature similarity graph constructor for POSIX_MEM_NOT_ALIGNED
2025-05-26 12:24:56,312 - INFO - Initialized feature similarity graph constructor for POSIX_FILE_NOT_ALIGNED
2025-05-26 12:24:56,312 - INFO - Initialized feature similarity graph constructor for POSIX_SEEKS
2025-05-26 12:24:56,312 - INFO - Initialized feature similarity graph constructor for POSIX_OPENS
2025-05-26 12:24:56,312 - INFO - Initialized multiplex graph constructor for 8 features
2025-05-26 12:24:56,312 - INFO - Creating combined PyG data
2025-05-26 12:24:56,313 - INFO - Loading precomputed similarity from data/similarity_output_merged_100K.pt
2025-05-26 12:25:56,420 - INFO - Splitting data into train, validation, and test sets
2025-05-26 12:25:56,607 - INFO - Train: 69999, Val: 15001, Test: 15000
2025-05-26 12:25:56,610 - INFO - Saving processed data to logs/training/all/Experiment4/combined/processed_data
2025-05-26 12:25:59,291 - INFO - Constructing multiplex graphs
2025-05-26 12:25:59,292 - INFO - Loading precomputed similarity from data/similarity_output_merged_100K.pt
2025-05-26 12:27:00,189 - INFO - Creating combined PyG data
2025-05-26 12:27:00,190 - INFO - Loading precomputed similarity from data/similarity_output_merged_100K.pt
2025-05-26 12:27:45,860 - INFO - Saved processed data to logs/training/all/Experiment4/combined/processed_data
2025-05-26 12:27:46,325 - INFO - Initialized GCN model with 2 layers
2025-05-26 12:27:46,325 - INFO - Initialized TabGNN regressor with 1 graph types
2025-05-26 12:27:47,248 - INFO - Epoch 1/100: Train Loss: 0.0171, Val Loss: 0.0083, Test Loss: 0.0078
2025-05-26 12:27:47,570 - INFO - Epoch 2/100: Train Loss: 0.0085, Val Loss: 0.0051, Test Loss: 0.0050
2025-05-26 12:27:47,873 - INFO - Epoch 3/100: Train Loss: 0.0054, Val Loss: 0.0043, Test Loss: 0.0043
2025-05-26 12:27:48,177 - INFO - Epoch 4/100: Train Loss: 0.0047, Val Loss: 0.0041, Test Loss: 0.0040
2025-05-26 12:27:48,481 - INFO - Epoch 5/100: Train Loss: 0.0043, Val Loss: 0.0033, Test Loss: 0.0033
2025-05-26 12:27:48,784 - INFO - Epoch 6/100: Train Loss: 0.0036, Val Loss: 0.0026, Test Loss: 0.0027
2025-05-26 12:27:49,089 - INFO - Epoch 7/100: Train Loss: 0.0031, Val Loss: 0.0024, Test Loss: 0.0024
2025-05-26 12:27:49,391 - INFO - Epoch 8/100: Train Loss: 0.0029, Val Loss: 0.0024, Test Loss: 0.0023
2025-05-26 12:27:49,694 - INFO - Epoch 9/100: Train Loss: 0.0030, Val Loss: 0.0021, Test Loss: 0.0021
2025-05-26 12:27:49,998 - INFO - Epoch 10/100: Train Loss: 0.0027, Val Loss: 0.0017, Test Loss: 0.0017
2025-05-26 12:27:50,301 - INFO - Epoch 11/100: Train Loss: 0.0023, Val Loss: 0.0015, Test Loss: 0.0016
2025-05-26 12:27:50,605 - INFO - Epoch 12/100: Train Loss: 0.0020, Val Loss: 0.0015, Test Loss: 0.0015
2025-05-26 12:27:50,909 - INFO - Epoch 13/100: Train Loss: 0.0018, Val Loss: 0.0015, Test Loss: 0.0015
2025-05-26 12:27:51,213 - INFO - Epoch 14/100: Train Loss: 0.0018, Val Loss: 0.0014, Test Loss: 0.0013
2025-05-26 12:27:51,516 - INFO - Epoch 15/100: Train Loss: 0.0016, Val Loss: 0.0011, Test Loss: 0.0011
2025-05-26 12:27:51,821 - INFO - Epoch 16/100: Train Loss: 0.0014, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 12:27:52,125 - INFO - Epoch 17/100: Train Loss: 0.0013, Val Loss: 0.0008, Test Loss: 0.0008
2025-05-26 12:27:52,428 - INFO - Epoch 18/100: Train Loss: 0.0013, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 12:27:52,731 - INFO - Epoch 19/100: Train Loss: 0.0013, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 12:27:53,035 - INFO - Epoch 20/100: Train Loss: 0.0013, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 12:27:53,339 - INFO - Epoch 21/100: Train Loss: 0.0013, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 12:27:53,642 - INFO - Epoch 22/100: Train Loss: 0.0013, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 12:27:53,946 - INFO - Epoch 23/100: Train Loss: 0.0013, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 12:27:54,250 - INFO - Epoch 24/100: Train Loss: 0.0012, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 12:27:54,554 - INFO - Epoch 25/100: Train Loss: 0.0012, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 12:27:54,858 - INFO - Epoch 26/100: Train Loss: 0.0012, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 12:27:55,162 - INFO - Epoch 27/100: Train Loss: 0.0011, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 12:27:55,162 - INFO - Early stopping at epoch 27
2025-05-26 12:27:55,163 - INFO - Initialized lightgbm model
2025-05-26 12:27:55,163 - INFO - Initialized TabGNN tabular model with lightgbm base model
/u/mbanisharifdehkordi/.conda/envs/gnn4_env/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.
  warnings.warn(
2025-05-26 12:28:12,993 - INFO - LightGBM model fitted with 109 features
2025-05-26 12:28:12,994 - INFO - TabGNN tabular model fitted with 109 features
/u/mbanisharifdehkordi/.conda/envs/gnn4_env/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.
  warnings.warn(
2025-05-26 12:28:13,554 - INFO - Evaluation metrics: {'mse': 5.1235127709854905e-05, 'rmse': 0.007157871730469533, 'mae': 0.0038557851193706, 'r2': 0.9786829684390286}
2025-05-26 12:28:13,559 - INFO - Final metrics: {'mse': 5.1235127709854905e-05, 'rmse': 0.007157871730469533, 'mae': 0.0038557851193706, 'r2': 0.9786829684390286}
2025-05-26 12:28:13,559 - INFO - Model and results saved to logs/training/all/Experiment4/combined
2025-05-26 12:28:13,564 - INFO - Model checkpoint saved to logs/training/all/Experiment4/combined/tabgnn_part.pt
2025-05-26 12:28:13,614 - INFO - Model saved to logs/training/all/Experiment4/combined/tabular_part.joblib
2025-05-26 12:28:13,614 - INFO - TabGNN tabular model saved to logs/training/all/Experiment4/combined/tabgnn_part.pt and logs/training/all/Experiment4/combined/tabular_part.joblib
Epoch 00023: reducing learning rate of group 0 to 5.0000e-04.
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010238 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 27792
[LightGBM] [Info] Number of data points in the train set: 100000, number of used features: 109
[LightGBM] [Info] Start training from score 0.037873
combined model completed.
