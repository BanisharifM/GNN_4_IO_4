Running tabgnn model...
2025-05-26 16:49:43,194 - INFO - Parsed arguments:
{
    "data_path": "data/10K/sample_10K.csv",
    "output_dir": "logs/training/all/Experiment5/tabgnn",
    "target_column": "tag",
    "important_features": "POSIX_SEQ_WRITES POSIX_SIZE_READ_1K_10K POSIX_SIZE_READ_0_100 POSIX_SIZE_WRITE_100K_1M POSIX_MEM_NOT_ALIGNED POSIX_FILE_NOT_ALIGNED POSIX_SEEKS POSIX_OPENS",
    "similarity_threshold": 0.05,
    "model_type": "tabgnn",
    "gnn_type": "gcn",
    "hidden_dim": 64,
    "num_layers": 2,
    "dropout": 0.1,
    "batch_size": 32,
    "epochs": 100,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "patience": 10,
    "seed": 42,
    "device": "cuda",
    "precomputed_similarity_path": "data/10K/similarity_output_10K.pt"
}
2025-05-26 16:49:43,206 - INFO - Using device: cuda
2025-05-26 16:49:43,206 - INFO - Initialized I/O data processor for data/10K/sample_10K.csv
2025-05-26 16:49:43,206 - INFO - Loading data from data/10K/sample_10K.csv
2025-05-26 16:49:43,264 - INFO - Loaded data with shape (10000, 46)
2025-05-26 16:49:43,264 - INFO - Preprocessing data
2025-05-26 16:49:43,264 - INFO - Initialized feature similarity graph constructor for POSIX_SEQ_WRITES
2025-05-26 16:49:43,264 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_1K_10K
2025-05-26 16:49:43,264 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_0_100
2025-05-26 16:49:43,264 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_WRITE_100K_1M
2025-05-26 16:49:43,264 - INFO - Initialized feature similarity graph constructor for POSIX_MEM_NOT_ALIGNED
2025-05-26 16:49:43,264 - INFO - Initialized feature similarity graph constructor for POSIX_FILE_NOT_ALIGNED
2025-05-26 16:49:43,264 - INFO - Initialized feature similarity graph constructor for POSIX_SEEKS
2025-05-26 16:49:43,264 - INFO - Initialized feature similarity graph constructor for POSIX_OPENS
2025-05-26 16:49:43,264 - INFO - Initialized multiplex graph constructor for 8 features
2025-05-26 16:49:43,264 - INFO - Creating combined PyG data
2025-05-26 16:49:43,264 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:49:43,856 - INFO - Splitting data into train, validation, and test sets
2025-05-26 16:49:43,909 - INFO - Train: 7000, Val: 1500, Test: 1500
2025-05-26 16:49:43,911 - INFO - Saving processed data to logs/training/all/Experiment5/tabgnn/processed_data
2025-05-26 16:49:44,114 - INFO - Constructing multiplex graphs
2025-05-26 16:49:44,114 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:49:44,542 - INFO - Creating combined PyG data
2025-05-26 16:49:44,543 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:49:45,018 - INFO - Saved processed data to logs/training/all/Experiment5/tabgnn/processed_data
2025-05-26 16:49:45,342 - INFO - Initialized GCN model with 2 layers
2025-05-26 16:49:45,342 - INFO - Initialized TabGNN regressor with 1 graph types
2025-05-26 16:49:46,534 - INFO - Epoch 1/100: Train Loss: 0.0070, Val Loss: 0.0047, Test Loss: 0.0048
2025-05-26 16:49:46,563 - INFO - Epoch 2/100: Train Loss: 0.0050, Val Loss: 0.0032, Test Loss: 0.0033
2025-05-26 16:49:46,567 - INFO - Epoch 3/100: Train Loss: 0.0035, Val Loss: 0.0025, Test Loss: 0.0026
2025-05-26 16:49:46,571 - INFO - Epoch 4/100: Train Loss: 0.0029, Val Loss: 0.0022, Test Loss: 0.0022
2025-05-26 16:49:46,575 - INFO - Epoch 5/100: Train Loss: 0.0025, Val Loss: 0.0019, Test Loss: 0.0020
2025-05-26 16:49:46,579 - INFO - Epoch 6/100: Train Loss: 0.0023, Val Loss: 0.0017, Test Loss: 0.0017
2025-05-26 16:49:46,582 - INFO - Epoch 7/100: Train Loss: 0.0022, Val Loss: 0.0015, Test Loss: 0.0015
2025-05-26 16:49:46,586 - INFO - Epoch 8/100: Train Loss: 0.0019, Val Loss: 0.0013, Test Loss: 0.0013
2025-05-26 16:49:46,590 - INFO - Epoch 9/100: Train Loss: 0.0016, Val Loss: 0.0012, Test Loss: 0.0012
2025-05-26 16:49:46,594 - INFO - Epoch 10/100: Train Loss: 0.0015, Val Loss: 0.0011, Test Loss: 0.0011
2025-05-26 16:49:46,598 - INFO - Epoch 11/100: Train Loss: 0.0014, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 16:49:46,602 - INFO - Epoch 12/100: Train Loss: 0.0013, Val Loss: 0.0008, Test Loss: 0.0008
2025-05-26 16:49:46,606 - INFO - Epoch 13/100: Train Loss: 0.0011, Val Loss: 0.0008, Test Loss: 0.0008
2025-05-26 16:49:46,610 - INFO - Epoch 14/100: Train Loss: 0.0012, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 16:49:46,614 - INFO - Epoch 15/100: Train Loss: 0.0012, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 16:49:46,618 - INFO - Epoch 16/100: Train Loss: 0.0013, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 16:49:46,621 - INFO - Epoch 17/100: Train Loss: 0.0013, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 16:49:46,625 - INFO - Epoch 18/100: Train Loss: 0.0012, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 16:49:46,629 - INFO - Epoch 19/100: Train Loss: 0.0012, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 16:49:46,633 - INFO - Epoch 20/100: Train Loss: 0.0011, Val Loss: 0.0008, Test Loss: 0.0009
2025-05-26 16:49:46,637 - INFO - Epoch 21/100: Train Loss: 0.0011, Val Loss: 0.0008, Test Loss: 0.0008
2025-05-26 16:49:46,641 - INFO - Epoch 22/100: Train Loss: 0.0011, Val Loss: 0.0008, Test Loss: 0.0008
2025-05-26 16:49:46,645 - INFO - Epoch 23/100: Train Loss: 0.0011, Val Loss: 0.0007, Test Loss: 0.0008
2025-05-26 16:49:46,649 - INFO - Epoch 24/100: Train Loss: 0.0010, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,652 - INFO - Epoch 25/100: Train Loss: 0.0010, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,656 - INFO - Epoch 26/100: Train Loss: 0.0010, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,660 - INFO - Epoch 27/100: Train Loss: 0.0010, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,664 - INFO - Epoch 28/100: Train Loss: 0.0010, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,668 - INFO - Epoch 29/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,672 - INFO - Epoch 30/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,676 - INFO - Epoch 31/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,680 - INFO - Epoch 32/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,684 - INFO - Epoch 33/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,688 - INFO - Epoch 34/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,691 - INFO - Epoch 35/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,695 - INFO - Epoch 36/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,699 - INFO - Epoch 37/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,703 - INFO - Epoch 38/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,707 - INFO - Epoch 39/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,711 - INFO - Epoch 40/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,715 - INFO - Epoch 41/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,719 - INFO - Epoch 42/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,722 - INFO - Epoch 43/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:49:46,726 - INFO - Epoch 44/100: Train Loss: 0.0009, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:49:46,730 - INFO - Epoch 45/100: Train Loss: 0.0009, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:49:46,734 - INFO - Epoch 46/100: Train Loss: 0.0009, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:49:46,738 - INFO - Epoch 47/100: Train Loss: 0.0009, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:49:46,742 - INFO - Epoch 48/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:49:46,746 - INFO - Epoch 49/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:49:46,750 - INFO - Epoch 50/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:49:46,754 - INFO - Epoch 51/100: Train Loss: 0.0009, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:49:46,758 - INFO - Epoch 52/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,761 - INFO - Epoch 53/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,765 - INFO - Epoch 54/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,769 - INFO - Epoch 55/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,773 - INFO - Epoch 56/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,777 - INFO - Epoch 57/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,781 - INFO - Epoch 58/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,785 - INFO - Epoch 59/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,789 - INFO - Epoch 60/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,793 - INFO - Epoch 61/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,797 - INFO - Epoch 62/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,801 - INFO - Epoch 63/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,805 - INFO - Epoch 64/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,808 - INFO - Epoch 65/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,812 - INFO - Epoch 66/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,816 - INFO - Epoch 67/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,820 - INFO - Epoch 68/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,824 - INFO - Epoch 69/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,828 - INFO - Epoch 70/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,832 - INFO - Epoch 71/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,836 - INFO - Epoch 72/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,840 - INFO - Epoch 73/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,844 - INFO - Epoch 74/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,847 - INFO - Epoch 75/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,851 - INFO - Epoch 76/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,855 - INFO - Epoch 77/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,859 - INFO - Epoch 78/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,863 - INFO - Epoch 79/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:49:46,867 - INFO - Epoch 80/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,871 - INFO - Epoch 81/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,875 - INFO - Epoch 82/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,879 - INFO - Epoch 83/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,882 - INFO - Epoch 84/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,886 - INFO - Epoch 85/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,890 - INFO - Epoch 86/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,894 - INFO - Epoch 87/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,898 - INFO - Epoch 88/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,902 - INFO - Epoch 89/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,906 - INFO - Epoch 90/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,910 - INFO - Epoch 91/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,914 - INFO - Epoch 92/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,918 - INFO - Epoch 93/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,921 - INFO - Epoch 94/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,925 - INFO - Epoch 95/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,929 - INFO - Epoch 96/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,933 - INFO - Epoch 97/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,937 - INFO - Epoch 98/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,941 - INFO - Epoch 99/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:49:46,945 - INFO - Epoch 100/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0005
2025-05-26 16:49:46,950 - INFO - Model checkpoint saved to logs/training/all/Experiment5/tabgnn/tabgnn_model.pt
2025-05-26 16:49:47,106 - INFO - Final metrics: {'mse': 0.0005497820093296468, 'rmse': 0.02344743076180516, 'mae': 0.014668375253677368, 'r2': 0.7740731239318848}
2025-05-26 16:49:47,106 - INFO - Model and results saved to logs/training/all/Experiment5/tabgnn
Epoch 00019: reducing learning rate of group 0 to 5.0000e-04.
Epoch 00037: reducing learning rate of group 0 to 2.5000e-04.
tabgnn model completed.
Running lightgbm model...
2025-05-26 16:49:53,950 - INFO - Parsed arguments:
{
    "data_path": "data/10K/sample_10K.csv",
    "output_dir": "logs/training/all/Experiment5/lightgbm",
    "target_column": "tag",
    "important_features": "POSIX_SEQ_WRITES POSIX_SIZE_READ_1K_10K POSIX_SIZE_READ_0_100 POSIX_SIZE_WRITE_100K_1M POSIX_MEM_NOT_ALIGNED POSIX_FILE_NOT_ALIGNED POSIX_SEEKS POSIX_OPENS",
    "similarity_threshold": 0.05,
    "model_type": "lightgbm",
    "gnn_type": "gcn",
    "hidden_dim": 64,
    "num_layers": 2,
    "dropout": 0.1,
    "batch_size": 32,
    "epochs": 100,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "patience": 10,
    "seed": 42,
    "device": "cuda",
    "precomputed_similarity_path": "data/10K/similarity_output_10K.pt"
}
2025-05-26 16:49:53,953 - INFO - Using device: cuda
2025-05-26 16:49:53,953 - INFO - Initialized I/O data processor for data/10K/sample_10K.csv
2025-05-26 16:49:53,953 - INFO - Loading data from data/10K/sample_10K.csv
2025-05-26 16:49:53,992 - INFO - Loaded data with shape (10000, 46)
2025-05-26 16:49:53,992 - INFO - Preprocessing data
2025-05-26 16:49:53,992 - INFO - Initialized feature similarity graph constructor for POSIX_SEQ_WRITES
2025-05-26 16:49:53,992 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_1K_10K
2025-05-26 16:49:53,992 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_0_100
2025-05-26 16:49:53,992 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_WRITE_100K_1M
2025-05-26 16:49:53,992 - INFO - Initialized feature similarity graph constructor for POSIX_MEM_NOT_ALIGNED
2025-05-26 16:49:53,992 - INFO - Initialized feature similarity graph constructor for POSIX_FILE_NOT_ALIGNED
2025-05-26 16:49:53,992 - INFO - Initialized feature similarity graph constructor for POSIX_SEEKS
2025-05-26 16:49:53,992 - INFO - Initialized feature similarity graph constructor for POSIX_OPENS
2025-05-26 16:49:53,992 - INFO - Initialized multiplex graph constructor for 8 features
2025-05-26 16:49:53,992 - INFO - Creating combined PyG data
2025-05-26 16:49:53,993 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:49:54,458 - INFO - Splitting data into train, validation, and test sets
2025-05-26 16:49:54,463 - INFO - Train: 7000, Val: 1500, Test: 1500
2025-05-26 16:49:54,468 - INFO - Saving processed data to logs/training/all/Experiment5/lightgbm/processed_data
2025-05-26 16:49:54,664 - INFO - Constructing multiplex graphs
2025-05-26 16:49:54,665 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:49:55,097 - INFO - Creating combined PyG data
2025-05-26 16:49:55,097 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:49:55,587 - INFO - Saved processed data to logs/training/all/Experiment5/lightgbm/processed_data
2025-05-26 16:49:55,588 - INFO - Initialized lightgbm model
/u/mbanisharifdehkordi/.conda/envs/gnn4_env/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.
  warnings.warn(
2025-05-26 16:49:57,134 - INFO - LightGBM model fitted with 45 features
/u/mbanisharifdehkordi/.conda/envs/gnn4_env/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.
  warnings.warn(
2025-05-26 16:49:57,144 - INFO - Evaluation metrics: {'mse': 0.00014865327618671301, 'rmse': 0.012192344983091359, 'mae': 0.0061898232131454, 'r2': 0.9389125745308208}
2025-05-26 16:49:57,147 - INFO - Final metrics: {'mse': 0.00014865327618671301, 'rmse': 0.012192344983091359, 'mae': 0.0061898232131454, 'r2': 0.9389125745308208}
2025-05-26 16:49:57,147 - INFO - Model and results saved to logs/training/all/Experiment5/lightgbm
2025-05-26 16:49:57,188 - INFO - Model saved to logs/training/all/Experiment5/lightgbm/lightgbm_model.joblib
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000613 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 11467
[LightGBM] [Info] Number of data points in the train set: 7000, number of used features: 45
[LightGBM] [Info] Start training from score 0.038149
lightgbm model completed.
Running catboost model...
2025-05-26 16:50:03,020 - INFO - Parsed arguments:
{
    "data_path": "data/10K/sample_10K.csv",
    "output_dir": "logs/training/all/Experiment5/catboost",
    "target_column": "tag",
    "important_features": "POSIX_SEQ_WRITES POSIX_SIZE_READ_1K_10K POSIX_SIZE_READ_0_100 POSIX_SIZE_WRITE_100K_1M POSIX_MEM_NOT_ALIGNED POSIX_FILE_NOT_ALIGNED POSIX_SEEKS POSIX_OPENS",
    "similarity_threshold": 0.05,
    "model_type": "catboost",
    "gnn_type": "gcn",
    "hidden_dim": 64,
    "num_layers": 2,
    "dropout": 0.1,
    "batch_size": 32,
    "epochs": 100,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "patience": 10,
    "seed": 42,
    "device": "cuda",
    "precomputed_similarity_path": "data/10K/similarity_output_10K.pt"
}
2025-05-26 16:50:03,023 - INFO - Using device: cuda
2025-05-26 16:50:03,023 - INFO - Initialized I/O data processor for data/10K/sample_10K.csv
2025-05-26 16:50:03,023 - INFO - Loading data from data/10K/sample_10K.csv
2025-05-26 16:50:03,059 - INFO - Loaded data with shape (10000, 46)
2025-05-26 16:50:03,059 - INFO - Preprocessing data
2025-05-26 16:50:03,059 - INFO - Initialized feature similarity graph constructor for POSIX_SEQ_WRITES
2025-05-26 16:50:03,059 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_1K_10K
2025-05-26 16:50:03,059 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_0_100
2025-05-26 16:50:03,059 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_WRITE_100K_1M
2025-05-26 16:50:03,059 - INFO - Initialized feature similarity graph constructor for POSIX_MEM_NOT_ALIGNED
2025-05-26 16:50:03,059 - INFO - Initialized feature similarity graph constructor for POSIX_FILE_NOT_ALIGNED
2025-05-26 16:50:03,059 - INFO - Initialized feature similarity graph constructor for POSIX_SEEKS
2025-05-26 16:50:03,059 - INFO - Initialized feature similarity graph constructor for POSIX_OPENS
2025-05-26 16:50:03,059 - INFO - Initialized multiplex graph constructor for 8 features
2025-05-26 16:50:03,059 - INFO - Creating combined PyG data
2025-05-26 16:50:03,060 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:50:03,547 - INFO - Splitting data into train, validation, and test sets
2025-05-26 16:50:03,556 - INFO - Train: 7000, Val: 1500, Test: 1500
2025-05-26 16:50:03,559 - INFO - Saving processed data to logs/training/all/Experiment5/catboost/processed_data
2025-05-26 16:50:03,764 - INFO - Constructing multiplex graphs
2025-05-26 16:50:03,764 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:50:04,217 - INFO - Creating combined PyG data
2025-05-26 16:50:04,217 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:50:04,719 - INFO - Saved processed data to logs/training/all/Experiment5/catboost/processed_data
2025-05-26 16:50:04,721 - INFO - Initialized catboost model
2025-05-26 16:50:06,825 - INFO - CatBoost model fitted with 45 features
2025-05-26 16:50:06,833 - INFO - Evaluation metrics: {'mse': 0.0001491045100843584, 'rmse': 0.012210835765186525, 'mae': 0.006499867671312642, 'r2': 0.9387271449338508}
2025-05-26 16:50:06,836 - INFO - Final metrics: {'mse': 0.0001491045100843584, 'rmse': 0.012210835765186525, 'mae': 0.006499867671312642, 'r2': 0.9387271449338508}
2025-05-26 16:50:06,836 - INFO - Model and results saved to logs/training/all/Experiment5/catboost
2025-05-26 16:50:06,846 - INFO - Model saved to logs/training/all/Experiment5/catboost/catboost_model.joblib
catboost model completed.
Running xgboost model...
2025-05-26 16:50:12,057 - INFO - Parsed arguments:
{
    "data_path": "data/10K/sample_10K.csv",
    "output_dir": "logs/training/all/Experiment5/xgboost",
    "target_column": "tag",
    "important_features": "POSIX_SEQ_WRITES POSIX_SIZE_READ_1K_10K POSIX_SIZE_READ_0_100 POSIX_SIZE_WRITE_100K_1M POSIX_MEM_NOT_ALIGNED POSIX_FILE_NOT_ALIGNED POSIX_SEEKS POSIX_OPENS",
    "similarity_threshold": 0.05,
    "model_type": "xgboost",
    "gnn_type": "gcn",
    "hidden_dim": 64,
    "num_layers": 2,
    "dropout": 0.1,
    "batch_size": 32,
    "epochs": 100,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "patience": 10,
    "seed": 42,
    "device": "cuda",
    "precomputed_similarity_path": "data/10K/similarity_output_10K.pt"
}
2025-05-26 16:50:12,059 - INFO - Using device: cuda
2025-05-26 16:50:12,059 - INFO - Initialized I/O data processor for data/10K/sample_10K.csv
2025-05-26 16:50:12,059 - INFO - Loading data from data/10K/sample_10K.csv
2025-05-26 16:50:12,096 - INFO - Loaded data with shape (10000, 46)
2025-05-26 16:50:12,096 - INFO - Preprocessing data
2025-05-26 16:50:12,096 - INFO - Initialized feature similarity graph constructor for POSIX_SEQ_WRITES
2025-05-26 16:50:12,096 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_1K_10K
2025-05-26 16:50:12,096 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_0_100
2025-05-26 16:50:12,096 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_WRITE_100K_1M
2025-05-26 16:50:12,096 - INFO - Initialized feature similarity graph constructor for POSIX_MEM_NOT_ALIGNED
2025-05-26 16:50:12,096 - INFO - Initialized feature similarity graph constructor for POSIX_FILE_NOT_ALIGNED
2025-05-26 16:50:12,096 - INFO - Initialized feature similarity graph constructor for POSIX_SEEKS
2025-05-26 16:50:12,096 - INFO - Initialized feature similarity graph constructor for POSIX_OPENS
2025-05-26 16:50:12,096 - INFO - Initialized multiplex graph constructor for 8 features
2025-05-26 16:50:12,096 - INFO - Creating combined PyG data
2025-05-26 16:50:12,096 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:50:12,567 - INFO - Splitting data into train, validation, and test sets
2025-05-26 16:50:12,576 - INFO - Train: 7000, Val: 1500, Test: 1500
2025-05-26 16:50:12,582 - INFO - Saving processed data to logs/training/all/Experiment5/xgboost/processed_data
2025-05-26 16:50:12,781 - INFO - Constructing multiplex graphs
2025-05-26 16:50:12,781 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:50:13,201 - INFO - Creating combined PyG data
2025-05-26 16:50:13,201 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:50:13,691 - INFO - Saved processed data to logs/training/all/Experiment5/xgboost/processed_data
2025-05-26 16:50:13,692 - INFO - Initialized xgboost model
2025-05-26 16:50:16,104 - INFO - XGBoost model fitted with 45 features
2025-05-26 16:50:16,112 - INFO - Evaluation metrics: {'mse': 0.00015150412218645215, 'rmse': 0.012308701076330197, 'mae': 0.0061679407954216, 'r2': 0.9377410411834717}
2025-05-26 16:50:16,114 - INFO - Final metrics: {'mse': 0.00015150412218645215, 'rmse': 0.012308701076330197, 'mae': 0.0061679407954216, 'r2': 0.9377410411834717}
2025-05-26 16:50:16,114 - INFO - Model and results saved to logs/training/all/Experiment5/xgboost
2025-05-26 16:50:16,136 - INFO - Model saved to logs/training/all/Experiment5/xgboost/xgboost_model.joblib
xgboost model completed.
Running mlp model...
2025-05-26 16:50:21,852 - INFO - Parsed arguments:
{
    "data_path": "data/10K/sample_10K.csv",
    "output_dir": "logs/training/all/Experiment5/mlp",
    "target_column": "tag",
    "important_features": "POSIX_SEQ_WRITES POSIX_SIZE_READ_1K_10K POSIX_SIZE_READ_0_100 POSIX_SIZE_WRITE_100K_1M POSIX_MEM_NOT_ALIGNED POSIX_FILE_NOT_ALIGNED POSIX_SEEKS POSIX_OPENS",
    "similarity_threshold": 0.05,
    "model_type": "mlp",
    "gnn_type": "gcn",
    "hidden_dim": 64,
    "num_layers": 2,
    "dropout": 0.1,
    "batch_size": 32,
    "epochs": 100,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "patience": 10,
    "seed": 42,
    "device": "cuda",
    "precomputed_similarity_path": "data/10K/similarity_output_10K.pt"
}
2025-05-26 16:50:21,855 - INFO - Using device: cuda
2025-05-26 16:50:21,855 - INFO - Initialized I/O data processor for data/10K/sample_10K.csv
2025-05-26 16:50:21,855 - INFO - Loading data from data/10K/sample_10K.csv
2025-05-26 16:50:21,894 - INFO - Loaded data with shape (10000, 46)
2025-05-26 16:50:21,894 - INFO - Preprocessing data
2025-05-26 16:50:21,894 - INFO - Initialized feature similarity graph constructor for POSIX_SEQ_WRITES
2025-05-26 16:50:21,894 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_1K_10K
2025-05-26 16:50:21,894 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_0_100
2025-05-26 16:50:21,894 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_WRITE_100K_1M
2025-05-26 16:50:21,894 - INFO - Initialized feature similarity graph constructor for POSIX_MEM_NOT_ALIGNED
2025-05-26 16:50:21,894 - INFO - Initialized feature similarity graph constructor for POSIX_FILE_NOT_ALIGNED
2025-05-26 16:50:21,894 - INFO - Initialized feature similarity graph constructor for POSIX_SEEKS
2025-05-26 16:50:21,894 - INFO - Initialized feature similarity graph constructor for POSIX_OPENS
2025-05-26 16:50:21,894 - INFO - Initialized multiplex graph constructor for 8 features
2025-05-26 16:50:21,894 - INFO - Creating combined PyG data
2025-05-26 16:50:21,894 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:50:22,362 - INFO - Splitting data into train, validation, and test sets
2025-05-26 16:50:22,370 - INFO - Train: 7000, Val: 1500, Test: 1500
2025-05-26 16:50:22,373 - INFO - Saving processed data to logs/training/all/Experiment5/mlp/processed_data
2025-05-26 16:50:22,568 - INFO - Constructing multiplex graphs
2025-05-26 16:50:22,568 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:50:22,999 - INFO - Creating combined PyG data
2025-05-26 16:50:23,000 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:50:23,497 - INFO - Saved processed data to logs/training/all/Experiment5/mlp/processed_data
2025-05-26 16:50:23,498 - INFO - Initialized mlp model
/u/mbanisharifdehkordi/.conda/envs/gnn4_env/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.
  warnings.warn(
2025-05-26 16:50:25,477 - INFO - MLP model fitted with 45 features
2025-05-26 16:50:25,479 - INFO - Evaluation metrics: {'mse': 0.0005447770818136632, 'rmse': 0.023340460188558048, 'mae': 0.012180229648947716, 'r2': 0.7761298418045044}
2025-05-26 16:50:25,483 - INFO - Final metrics: {'mse': 0.0005447770818136632, 'rmse': 0.023340460188558048, 'mae': 0.012180229648947716, 'r2': 0.7761298418045044}
2025-05-26 16:50:25,483 - INFO - Model and results saved to logs/training/all/Experiment5/mlp
2025-05-26 16:50:25,490 - INFO - Model saved to logs/training/all/Experiment5/mlp/mlp_model.joblib
mlp model completed.
Running tabnet model...
2025-05-26 16:50:29,966 - INFO - Parsed arguments:
{
    "data_path": "data/10K/sample_10K.csv",
    "output_dir": "logs/training/all/Experiment5/tabnet",
    "target_column": "tag",
    "important_features": "POSIX_SEQ_WRITES POSIX_SIZE_READ_1K_10K POSIX_SIZE_READ_0_100 POSIX_SIZE_WRITE_100K_1M POSIX_MEM_NOT_ALIGNED POSIX_FILE_NOT_ALIGNED POSIX_SEEKS POSIX_OPENS",
    "similarity_threshold": 0.05,
    "model_type": "tabnet",
    "gnn_type": "gcn",
    "hidden_dim": 64,
    "num_layers": 2,
    "dropout": 0.1,
    "batch_size": 32,
    "epochs": 100,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "patience": 10,
    "seed": 42,
    "device": "cuda",
    "precomputed_similarity_path": "data/10K/similarity_output_10K.pt"
}
2025-05-26 16:50:29,968 - INFO - Using device: cuda
2025-05-26 16:50:29,968 - INFO - Initialized I/O data processor for data/10K/sample_10K.csv
2025-05-26 16:50:29,968 - INFO - Loading data from data/10K/sample_10K.csv
2025-05-26 16:50:30,004 - INFO - Loaded data with shape (10000, 46)
2025-05-26 16:50:30,004 - INFO - Preprocessing data
2025-05-26 16:50:30,004 - INFO - Initialized feature similarity graph constructor for POSIX_SEQ_WRITES
2025-05-26 16:50:30,004 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_1K_10K
2025-05-26 16:50:30,004 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_0_100
2025-05-26 16:50:30,004 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_WRITE_100K_1M
2025-05-26 16:50:30,004 - INFO - Initialized feature similarity graph constructor for POSIX_MEM_NOT_ALIGNED
2025-05-26 16:50:30,004 - INFO - Initialized feature similarity graph constructor for POSIX_FILE_NOT_ALIGNED
2025-05-26 16:50:30,004 - INFO - Initialized feature similarity graph constructor for POSIX_SEEKS
2025-05-26 16:50:30,004 - INFO - Initialized feature similarity graph constructor for POSIX_OPENS
2025-05-26 16:50:30,004 - INFO - Initialized multiplex graph constructor for 8 features
2025-05-26 16:50:30,004 - INFO - Creating combined PyG data
2025-05-26 16:50:30,004 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:50:30,455 - INFO - Splitting data into train, validation, and test sets
2025-05-26 16:50:30,463 - INFO - Train: 7000, Val: 1500, Test: 1500
2025-05-26 16:50:30,469 - INFO - Saving processed data to logs/training/all/Experiment5/tabnet/processed_data
2025-05-26 16:50:30,664 - INFO - Constructing multiplex graphs
2025-05-26 16:50:30,664 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:50:31,085 - INFO - Creating combined PyG data
2025-05-26 16:50:31,085 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:50:31,577 - INFO - Saved processed data to logs/training/all/Experiment5/tabnet/processed_data
2025-05-26 16:50:31,578 - INFO - Initialized tabnet model
/u/mbanisharifdehkordi/.local/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda
  warnings.warn(f"Device used : {self.device}")
/u/mbanisharifdehkordi/.local/lib/python3.9/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.
  warnings.warn(wrn_msg)
2025-05-26 16:50:42,967 - INFO - TabNet model fitted with 45 features
2025-05-26 16:50:42,983 - INFO - Evaluation metrics: {'mse': 0.004037569742649794, 'rmse': 0.06354187393089532, 'mae': 0.03923201188445091, 'r2': -0.6591947078704834}
2025-05-26 16:50:42,987 - INFO - Final metrics: {'mse': 0.004037569742649794, 'rmse': 0.06354187393089532, 'mae': 0.03923201188445091, 'r2': -0.6591947078704834}
2025-05-26 16:50:42,987 - INFO - Model and results saved to logs/training/all/Experiment5/tabnet
2025-05-26 16:50:43,032 - INFO - Model saved to logs/training/all/Experiment5/tabnet/tabnet_model.joblib
epoch 0  | loss: 10.98163|  0:00:00s
epoch 1  | loss: 4.44806 |  0:00:00s
epoch 2  | loss: 1.6966  |  0:00:01s
epoch 3  | loss: 1.02974 |  0:00:01s
epoch 4  | loss: 0.84413 |  0:00:01s
epoch 5  | loss: 0.59838 |  0:00:01s
epoch 6  | loss: 0.48256 |  0:00:01s
epoch 7  | loss: 0.37364 |  0:00:01s
epoch 8  | loss: 0.30514 |  0:00:01s
epoch 9  | loss: 0.26719 |  0:00:01s
epoch 10 | loss: 0.22013 |  0:00:01s
epoch 11 | loss: 0.19416 |  0:00:02s
epoch 12 | loss: 0.17378 |  0:00:02s
epoch 13 | loss: 0.15326 |  0:00:02s
epoch 14 | loss: 0.16288 |  0:00:02s
epoch 15 | loss: 0.12729 |  0:00:02s
epoch 16 | loss: 0.11465 |  0:00:02s
epoch 17 | loss: 0.10808 |  0:00:02s
epoch 18 | loss: 0.09273 |  0:00:02s
epoch 19 | loss: 0.08563 |  0:00:02s
epoch 20 | loss: 0.07887 |  0:00:02s
epoch 21 | loss: 0.06787 |  0:00:03s
epoch 22 | loss: 0.0577  |  0:00:03s
epoch 23 | loss: 0.05771 |  0:00:03s
epoch 24 | loss: 0.0513  |  0:00:03s
epoch 25 | loss: 0.04593 |  0:00:03s
epoch 26 | loss: 0.04346 |  0:00:03s
epoch 27 | loss: 0.04144 |  0:00:03s
epoch 28 | loss: 0.04176 |  0:00:03s
epoch 29 | loss: 0.03911 |  0:00:03s
epoch 30 | loss: 0.031   |  0:00:03s
epoch 31 | loss: 0.02872 |  0:00:04s
epoch 32 | loss: 0.02628 |  0:00:04s
epoch 33 | loss: 0.02899 |  0:00:04s
epoch 34 | loss: 0.02612 |  0:00:04s
epoch 35 | loss: 0.02297 |  0:00:04s
epoch 36 | loss: 0.02247 |  0:00:04s
epoch 37 | loss: 0.02074 |  0:00:04s
epoch 38 | loss: 0.02055 |  0:00:04s
epoch 39 | loss: 0.02169 |  0:00:04s
epoch 40 | loss: 0.01941 |  0:00:05s
epoch 41 | loss: 0.02071 |  0:00:05s
epoch 42 | loss: 0.01888 |  0:00:05s
epoch 43 | loss: 0.01777 |  0:00:05s
epoch 44 | loss: 0.01744 |  0:00:05s
epoch 45 | loss: 0.01752 |  0:00:05s
epoch 46 | loss: 0.01825 |  0:00:05s
epoch 47 | loss: 0.01646 |  0:00:05s
epoch 48 | loss: 0.01794 |  0:00:05s
epoch 49 | loss: 0.01506 |  0:00:05s
epoch 50 | loss: 0.01697 |  0:00:06s
epoch 51 | loss: 0.01767 |  0:00:06s
epoch 52 | loss: 0.01594 |  0:00:06s
epoch 53 | loss: 0.01575 |  0:00:06s
epoch 54 | loss: 0.01611 |  0:00:06s
epoch 55 | loss: 0.01434 |  0:00:06s
epoch 56 | loss: 0.01332 |  0:00:06s
epoch 57 | loss: 0.01248 |  0:00:06s
epoch 58 | loss: 0.01126 |  0:00:06s
epoch 59 | loss: 0.01252 |  0:00:06s
epoch 60 | loss: 0.01155 |  0:00:07s
epoch 61 | loss: 0.01172 |  0:00:07s
epoch 62 | loss: 0.01013 |  0:00:07s
epoch 63 | loss: 0.0105  |  0:00:07s
epoch 64 | loss: 0.01122 |  0:00:07s
epoch 65 | loss: 0.01106 |  0:00:07s
epoch 66 | loss: 0.01125 |  0:00:07s
epoch 67 | loss: 0.01057 |  0:00:07s
epoch 68 | loss: 0.01038 |  0:00:07s
epoch 69 | loss: 0.01124 |  0:00:07s
epoch 70 | loss: 0.00903 |  0:00:08s
epoch 71 | loss: 0.01042 |  0:00:08s
epoch 72 | loss: 0.00968 |  0:00:08s
epoch 73 | loss: 0.00861 |  0:00:08s
epoch 74 | loss: 0.00841 |  0:00:08s
epoch 75 | loss: 0.00784 |  0:00:08s
epoch 76 | loss: 0.00862 |  0:00:08s
epoch 77 | loss: 0.00808 |  0:00:08s
epoch 78 | loss: 0.00795 |  0:00:08s
epoch 79 | loss: 0.00905 |  0:00:09s
epoch 80 | loss: 0.00876 |  0:00:09s
epoch 81 | loss: 0.00866 |  0:00:09s
epoch 82 | loss: 0.00826 |  0:00:09s
epoch 83 | loss: 0.00737 |  0:00:09s
epoch 84 | loss: 0.00816 |  0:00:09s
epoch 85 | loss: 0.00758 |  0:00:09s
epoch 86 | loss: 0.00823 |  0:00:09s
epoch 87 | loss: 0.00711 |  0:00:09s
epoch 88 | loss: 0.00791 |  0:00:09s
epoch 89 | loss: 0.00751 |  0:00:10s
epoch 90 | loss: 0.00647 |  0:00:10s
epoch 91 | loss: 0.00784 |  0:00:10s
epoch 92 | loss: 0.00798 |  0:00:10s
epoch 93 | loss: 0.00814 |  0:00:10s
epoch 94 | loss: 0.00779 |  0:00:10s
epoch 95 | loss: 0.00774 |  0:00:10s
epoch 96 | loss: 0.00754 |  0:00:10s
epoch 97 | loss: 0.00721 |  0:00:10s
epoch 98 | loss: 0.00652 |  0:00:10s
epoch 99 | loss: 0.00753 |  0:00:11s
tabnet model completed.
Running combined model...
2025-05-26 16:50:49,252 - INFO - Parsed arguments:
{
    "data_path": "data/10K/sample_10K.csv",
    "output_dir": "logs/training/all/Experiment5/combined",
    "target_column": "tag",
    "important_features": "POSIX_SEQ_WRITES POSIX_SIZE_READ_1K_10K POSIX_SIZE_READ_0_100 POSIX_SIZE_WRITE_100K_1M POSIX_MEM_NOT_ALIGNED POSIX_FILE_NOT_ALIGNED POSIX_SEEKS POSIX_OPENS",
    "similarity_threshold": 0.05,
    "model_type": "combined",
    "gnn_type": "gcn",
    "hidden_dim": 64,
    "num_layers": 2,
    "dropout": 0.1,
    "batch_size": 32,
    "epochs": 100,
    "lr": 0.001,
    "weight_decay": 0.0001,
    "patience": 10,
    "seed": 42,
    "device": "cuda",
    "precomputed_similarity_path": "data/10K/similarity_output_10K.pt"
}
2025-05-26 16:50:49,254 - INFO - Using device: cuda
2025-05-26 16:50:49,254 - INFO - Initialized I/O data processor for data/10K/sample_10K.csv
2025-05-26 16:50:49,254 - INFO - Loading data from data/10K/sample_10K.csv
2025-05-26 16:50:49,292 - INFO - Loaded data with shape (10000, 46)
2025-05-26 16:50:49,292 - INFO - Preprocessing data
2025-05-26 16:50:49,292 - INFO - Initialized feature similarity graph constructor for POSIX_SEQ_WRITES
2025-05-26 16:50:49,292 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_1K_10K
2025-05-26 16:50:49,292 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_READ_0_100
2025-05-26 16:50:49,292 - INFO - Initialized feature similarity graph constructor for POSIX_SIZE_WRITE_100K_1M
2025-05-26 16:50:49,292 - INFO - Initialized feature similarity graph constructor for POSIX_MEM_NOT_ALIGNED
2025-05-26 16:50:49,292 - INFO - Initialized feature similarity graph constructor for POSIX_FILE_NOT_ALIGNED
2025-05-26 16:50:49,292 - INFO - Initialized feature similarity graph constructor for POSIX_SEEKS
2025-05-26 16:50:49,292 - INFO - Initialized feature similarity graph constructor for POSIX_OPENS
2025-05-26 16:50:49,292 - INFO - Initialized multiplex graph constructor for 8 features
2025-05-26 16:50:49,292 - INFO - Creating combined PyG data
2025-05-26 16:50:49,293 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:50:49,751 - INFO - Splitting data into train, validation, and test sets
2025-05-26 16:50:49,760 - INFO - Train: 7000, Val: 1500, Test: 1500
2025-05-26 16:50:49,765 - INFO - Saving processed data to logs/training/all/Experiment5/combined/processed_data
2025-05-26 16:50:49,967 - INFO - Constructing multiplex graphs
2025-05-26 16:50:49,967 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:50:50,384 - INFO - Creating combined PyG data
2025-05-26 16:50:50,384 - INFO - Loading precomputed similarity from data/10K/similarity_output_10K.pt
2025-05-26 16:50:50,877 - INFO - Saved processed data to logs/training/all/Experiment5/combined/processed_data
2025-05-26 16:50:51,145 - INFO - Initialized GCN model with 2 layers
2025-05-26 16:50:51,145 - INFO - Initialized TabGNN regressor with 1 graph types
2025-05-26 16:50:51,313 - INFO - Epoch 1/100: Train Loss: 0.0070, Val Loss: 0.0047, Test Loss: 0.0048
2025-05-26 16:50:51,320 - INFO - Epoch 2/100: Train Loss: 0.0050, Val Loss: 0.0032, Test Loss: 0.0033
2025-05-26 16:50:51,324 - INFO - Epoch 3/100: Train Loss: 0.0035, Val Loss: 0.0025, Test Loss: 0.0026
2025-05-26 16:50:51,328 - INFO - Epoch 4/100: Train Loss: 0.0029, Val Loss: 0.0022, Test Loss: 0.0022
2025-05-26 16:50:51,332 - INFO - Epoch 5/100: Train Loss: 0.0025, Val Loss: 0.0019, Test Loss: 0.0020
2025-05-26 16:50:51,336 - INFO - Epoch 6/100: Train Loss: 0.0023, Val Loss: 0.0017, Test Loss: 0.0017
2025-05-26 16:50:51,340 - INFO - Epoch 7/100: Train Loss: 0.0022, Val Loss: 0.0015, Test Loss: 0.0015
2025-05-26 16:50:51,344 - INFO - Epoch 8/100: Train Loss: 0.0019, Val Loss: 0.0013, Test Loss: 0.0013
2025-05-26 16:50:51,348 - INFO - Epoch 9/100: Train Loss: 0.0016, Val Loss: 0.0012, Test Loss: 0.0012
2025-05-26 16:50:51,352 - INFO - Epoch 10/100: Train Loss: 0.0015, Val Loss: 0.0011, Test Loss: 0.0011
2025-05-26 16:50:51,356 - INFO - Epoch 11/100: Train Loss: 0.0014, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 16:50:51,359 - INFO - Epoch 12/100: Train Loss: 0.0013, Val Loss: 0.0008, Test Loss: 0.0008
2025-05-26 16:50:51,363 - INFO - Epoch 13/100: Train Loss: 0.0011, Val Loss: 0.0008, Test Loss: 0.0008
2025-05-26 16:50:51,367 - INFO - Epoch 14/100: Train Loss: 0.0012, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 16:50:51,371 - INFO - Epoch 15/100: Train Loss: 0.0012, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 16:50:51,375 - INFO - Epoch 16/100: Train Loss: 0.0013, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 16:50:51,379 - INFO - Epoch 17/100: Train Loss: 0.0013, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 16:50:51,383 - INFO - Epoch 18/100: Train Loss: 0.0012, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 16:50:51,387 - INFO - Epoch 19/100: Train Loss: 0.0012, Val Loss: 0.0009, Test Loss: 0.0009
2025-05-26 16:50:51,391 - INFO - Epoch 20/100: Train Loss: 0.0011, Val Loss: 0.0008, Test Loss: 0.0009
2025-05-26 16:50:51,394 - INFO - Epoch 21/100: Train Loss: 0.0011, Val Loss: 0.0008, Test Loss: 0.0008
2025-05-26 16:50:51,398 - INFO - Epoch 22/100: Train Loss: 0.0011, Val Loss: 0.0008, Test Loss: 0.0008
2025-05-26 16:50:51,402 - INFO - Epoch 23/100: Train Loss: 0.0011, Val Loss: 0.0007, Test Loss: 0.0008
2025-05-26 16:50:51,406 - INFO - Epoch 24/100: Train Loss: 0.0010, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,410 - INFO - Epoch 25/100: Train Loss: 0.0010, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,414 - INFO - Epoch 26/100: Train Loss: 0.0010, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,418 - INFO - Epoch 27/100: Train Loss: 0.0010, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,422 - INFO - Epoch 28/100: Train Loss: 0.0010, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,426 - INFO - Epoch 29/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,430 - INFO - Epoch 30/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,434 - INFO - Epoch 31/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,438 - INFO - Epoch 32/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,442 - INFO - Epoch 33/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,445 - INFO - Epoch 34/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,449 - INFO - Epoch 35/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,453 - INFO - Epoch 36/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,457 - INFO - Epoch 37/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,461 - INFO - Epoch 38/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,465 - INFO - Epoch 39/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,469 - INFO - Epoch 40/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,473 - INFO - Epoch 41/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,476 - INFO - Epoch 42/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,480 - INFO - Epoch 43/100: Train Loss: 0.0009, Val Loss: 0.0007, Test Loss: 0.0007
2025-05-26 16:50:51,484 - INFO - Epoch 44/100: Train Loss: 0.0009, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:50:51,488 - INFO - Epoch 45/100: Train Loss: 0.0009, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:50:51,492 - INFO - Epoch 46/100: Train Loss: 0.0009, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:50:51,496 - INFO - Epoch 47/100: Train Loss: 0.0009, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:50:51,500 - INFO - Epoch 48/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:50:51,504 - INFO - Epoch 49/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:50:51,508 - INFO - Epoch 50/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:50:51,512 - INFO - Epoch 51/100: Train Loss: 0.0009, Val Loss: 0.0006, Test Loss: 0.0007
2025-05-26 16:50:51,516 - INFO - Epoch 52/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,520 - INFO - Epoch 53/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,524 - INFO - Epoch 54/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,528 - INFO - Epoch 55/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,532 - INFO - Epoch 56/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,535 - INFO - Epoch 57/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,539 - INFO - Epoch 58/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,543 - INFO - Epoch 59/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,547 - INFO - Epoch 60/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,551 - INFO - Epoch 61/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,555 - INFO - Epoch 62/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,559 - INFO - Epoch 63/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,563 - INFO - Epoch 64/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,567 - INFO - Epoch 65/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,571 - INFO - Epoch 66/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,575 - INFO - Epoch 67/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,579 - INFO - Epoch 68/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,583 - INFO - Epoch 69/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,587 - INFO - Epoch 70/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,590 - INFO - Epoch 71/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,594 - INFO - Epoch 72/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,598 - INFO - Epoch 73/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,602 - INFO - Epoch 74/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,606 - INFO - Epoch 75/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,610 - INFO - Epoch 76/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,614 - INFO - Epoch 77/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,618 - INFO - Epoch 78/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,622 - INFO - Epoch 79/100: Train Loss: 0.0008, Val Loss: 0.0006, Test Loss: 0.0006
2025-05-26 16:50:51,626 - INFO - Epoch 80/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,630 - INFO - Epoch 81/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,634 - INFO - Epoch 82/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,637 - INFO - Epoch 83/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,641 - INFO - Epoch 84/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,645 - INFO - Epoch 85/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,649 - INFO - Epoch 86/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,653 - INFO - Epoch 87/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,657 - INFO - Epoch 88/100: Train Loss: 0.0008, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,661 - INFO - Epoch 89/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,665 - INFO - Epoch 90/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,669 - INFO - Epoch 91/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,673 - INFO - Epoch 92/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,677 - INFO - Epoch 93/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,681 - INFO - Epoch 94/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,684 - INFO - Epoch 95/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,688 - INFO - Epoch 96/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,692 - INFO - Epoch 97/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,696 - INFO - Epoch 98/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,700 - INFO - Epoch 99/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0006
2025-05-26 16:50:51,704 - INFO - Epoch 100/100: Train Loss: 0.0007, Val Loss: 0.0005, Test Loss: 0.0005
2025-05-26 16:50:51,705 - INFO - Initialized lightgbm model
2025-05-26 16:50:51,705 - INFO - Initialized TabGNN tabular model with lightgbm base model
/u/mbanisharifdehkordi/.conda/envs/gnn4_env/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.
  warnings.warn(
2025-05-26 16:50:53,837 - INFO - LightGBM model fitted with 109 features
2025-05-26 16:50:53,837 - INFO - TabGNN tabular model fitted with 109 features
/u/mbanisharifdehkordi/.conda/envs/gnn4_env/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.
  warnings.warn(
2025-05-26 16:50:53,879 - INFO - Evaluation metrics: {'mse': 9.100160013704909e-06, 'rmse': 0.0030166471476964136, 'mae': 0.0016477382659521424, 'r2': 0.9962912197288676}
2025-05-26 16:50:53,883 - INFO - Final metrics: {'mse': 9.100160013704909e-06, 'rmse': 0.0030166471476964136, 'mae': 0.0016477382659521424, 'r2': 0.9962912197288676}
2025-05-26 16:50:53,883 - INFO - Model and results saved to logs/training/all/Experiment5/combined
2025-05-26 16:50:53,887 - INFO - Model checkpoint saved to logs/training/all/Experiment5/combined/tabgnn_part.pt
2025-05-26 16:50:53,927 - INFO - Model saved to logs/training/all/Experiment5/combined/tabular_part.joblib
2025-05-26 16:50:53,927 - INFO - TabGNN tabular model saved to logs/training/all/Experiment5/combined/tabgnn_part.pt and logs/training/all/Experiment5/combined/tabular_part.joblib
Epoch 00019: reducing learning rate of group 0 to 5.0000e-04.
Epoch 00037: reducing learning rate of group 0 to 2.5000e-04.
[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001633 seconds.
You can set `force_col_wise=true` to remove the overhead.
[LightGBM] [Info] Total Bins 27787
[LightGBM] [Info] Number of data points in the train set: 10000, number of used features: 109
[LightGBM] [Info] Start training from score 0.037983
combined model completed.
2025-05-26 16:50:55,228 - INFO - Loaded metrics for catboost: {'mse': 0.0001491045100843584, 'rmse': 0.012210835765186525, 'mae': 0.006499867671312642, 'r2': 0.9387271449338508}
2025-05-26 16:50:55,229 - INFO - Loaded metrics for combined: {'mse': 9.100160013704909e-06, 'rmse': 0.0030166471476964136, 'mae': 0.0016477382659521424, 'r2': 0.9962912197288676}
2025-05-26 16:50:55,230 - INFO - Loaded metrics for lightgbm: {'mse': 0.00014865327618671301, 'rmse': 0.012192344983091359, 'mae': 0.0061898232131454, 'r2': 0.9389125745308208}
2025-05-26 16:50:55,230 - INFO - Loaded metrics for mlp: {'mse': 0.0005447770818136632, 'rmse': 0.023340460188558048, 'mae': 0.012180229648947716, 'r2': 0.7761298418045044}
2025-05-26 16:50:55,232 - INFO - Loaded metrics for tabgnn: {'mse': 0.0005497820093296468, 'rmse': 0.02344743076180516, 'mae': 0.014668375253677368, 'r2': 0.7740731239318848}
2025-05-26 16:50:55,232 - INFO - Loaded metrics for tabnet: {'mse': 0.004037569742649794, 'rmse': 0.06354187393089532, 'mae': 0.03923201188445091, 'r2': -0.6591947078704834}
2025-05-26 16:50:55,235 - INFO - Loaded metrics for xgboost: {'mse': 0.00015150412218645215, 'rmse': 0.012308701076330197, 'mae': 0.0061679407954216, 'r2': 0.9377410411834717}
2025-05-26 16:50:55,260 - INFO - Comparison report saved to logs/training/comparison_report_Experiment5.json
2025-05-26 16:50:55,260 - INFO - Markdown summary saved to logs/training/comparison_report_Experiment5.md
2025-05-26 16:50:55,374 - INFO - Saved RMSE plot to logs/training/model_comparison_Experiment5_rmse.png
2025-05-26 16:50:55,469 - INFO - Saved MAE plot to logs/training/model_comparison_Experiment5_mae.png
2025-05-26 16:50:55,555 - INFO - Saved R2 plot to logs/training/model_comparison_Experiment5_r2.png
2025-05-26 16:50:55,555 - INFO - Best model: combined
2025-05-26 16:50:55,555 - INFO - Best metrics: {'mse': 9.100160013704909e-06, 'rmse': 0.0030166471476964136, 'mae': 0.0016477382659521424, 'r2': 0.9962912197288676, 'improvement': 75.25785932173036}
All jobs completed at Mon May 26 16:50:55 CDT 2025
